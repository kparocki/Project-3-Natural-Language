{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Natural Language Proccessing\n",
    "---\n",
    "\n",
    "**Group C2: Erling Amundsen, Christian Hetling, Alexander Nielsen, Malthe Musaeus, Krzysztof Parocki**\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook you will find all our code and methods used for our pdf hand-in. Our focus was on detecting hate speech in tweets in the binary case, and detecting sentiment in tweets in the multi class case. The hate speech tweets and sentiment tweets were taken from the TweetEval corpus - train, test and evaluation data. \n",
    "[TweetEval](https://github.com/cardiffnlp/tweeteval#evaluating-your-system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.lm import WittenBellInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
    "from nltk.util import bigrams, trigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tokenizer_selfmade\n",
    "from utils import load_augmented_df, load_to_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "Natural language models (NLM) are one of the largest sectors in the maching learning community. NLM's are largely used in social media as a tool to stop spreading of misinformation and hatespeech, and in trading bots to evaluate news about stocks. There is a debate on the responsibilites companies have to censor their platforms, and to what extent. Our project focuses solely on correctly labelling tweets from the [TweetEval](https://github.com/cardiffnlp/tweeteval#evaluating-your-system) dataset, and calculates scores only on the basis of the given classification data, not any biases we might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the datasets\n",
    "---\n",
    "The two datasets we have chosen were hate and sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Choosing the dataset\n",
    "dataset1 = 'hate'\n",
    "dataset2 = 'sentiment'\n",
    "splits = ['test','train','val']\n",
    "texts = [dataset1,dataset2]\n",
    "\n",
    "# Loading the datasets and corresponding labels into panda dataframes\n",
    "def exporting_data(dataset):\n",
    "    train_df = load_to_pd('train', dataset)\n",
    "    val_df = load_to_pd('val', dataset)\n",
    "    test_df = load_to_pd('test', dataset)\n",
    "\n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "\n",
    "train_df_hate, test_df_hate, val_df_hate = exporting_data(dataset1)\n",
    "train_df_sentiment, test_df_sentiment, val_df_sentiment = exporting_data(dataset2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing the train_text_df\n",
    "def load_pickle_dataset(dataset):\n",
    "    if not exists(f\"data/tokens/{dataset}-tokens.pickle\"):\n",
    "        train_tokens, unmatchable = tokenizer_selfmade.tokenize_file(f'data/{dataset}/train_text.txt', unmatch=True)\n",
    "\n",
    "        with open(f\"data/tokens/{dataset}-tokens.pickle\", \"wb\") as f:\n",
    "            pickle.dump(train_tokens_hate, f)\n",
    "\n",
    "    with open(f\"data/tokens/{dataset}-tokens.pickle\", \"rb\") as f:\n",
    "        train_tokens = pickle.load(f)\n",
    "\n",
    "    return train_tokens\n",
    "\n",
    "\n",
    "# Tokenization with selfmade tokenizer\n",
    "train_tokens_hate = load_pickle_dataset(dataset1)\n",
    "train_tokens_sentiment = load_pickle_dataset(dataset2)\n",
    "\n",
    "\n",
    "# Tweet tokenizer \n",
    "tk = TweetTokenizer()\n",
    "tweet_tokens = train_df_hate['text'].apply(tk.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for our, their in zip(train_tokens_hate['tokens'], tweet_tokens):\n",
    "    set_our = set(our)\n",
    "    set_their = set(their)\n",
    "    diff = set.difference(set_our, set_their)\n",
    "    if diff:\n",
    "        print(diff)\n",
    "        print(f'{\" \".join(our)} \\n{\" \".join(their)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tokens_hate[:10], '\\n')\n",
    "print(tweet_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution\n",
    "\n",
    "In this section we calculate the distribution of labels in the datasets as a part of the pre-analysis. It is important for any dataset to have a reasonable representation of all classes, and for the dataset to be large enough. Our datasets are arguably small in size when compared to the massive amount of data that could be attained - millions of tweets, as opposed to thousands. However, it's definitely big enough for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculates the distribution of the labels, the figures are used in the document hand in\n",
    "texts = [dataset1,dataset2]\n",
    "\n",
    "sum_list = []\n",
    "len_list = []\n",
    "\n",
    "\n",
    "for y in splits:\n",
    "    _sum = 0\n",
    "    len_ = 0\n",
    "    with open(f'data/hate/{y}_labels.txt') as labels:\n",
    "        for i in labels:\n",
    "            _sum += int(i)\n",
    "            len_ += 1\n",
    "    sum_list.append(_sum)\n",
    "    len_list.append(len_)\n",
    "        \n",
    "print('hate: \\n')\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'{splits[i]} hate: {sum_list[i]} %: {(sum_list[i]/len_list[i]) * 100 : .3f} \\nnon-hate: {len_list[i] - sum_list[i]} %: {(1 - sum_list[i]/len_list[i]) * 100 : .3f} \\n')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "zero = [0,0,0]\n",
    "one = [0,0,0]\n",
    "two = [0,0,0]\n",
    "len_list2 = [0,0,0]\n",
    "\n",
    "index = 0\n",
    "for y in splits:\n",
    "    with open(f'data/sentiment/{y}_labels.txt') as labels:\n",
    "        for i in labels:\n",
    "            if int(i) == 0:\n",
    "                zero[index] +=1 \n",
    "            if int(i) == 1:\n",
    "                one[index] +=1\n",
    "            if int(i) == 2:\n",
    "                two[index] += 1\n",
    "            len_list2[index] += 1\n",
    "    index += 1\n",
    "\n",
    "print('sentiemnt:')\n",
    "for i in range(3):\n",
    "    print(f'{splits[i]} negative count: {zero[i]} %: {(zero[i]/len_list2[i])*100 : .3f}')\n",
    "    print(f'{splits[i]} neutral count: {one[i]} %: {(one[i]/len_list2[i])*100 : .3f}')\n",
    "    print(f'{splits[i]} positive count: {two[i]} %: {(two[i]/len_list2[i])*100 : .3f} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above code, it can be seen that the distribution between hate and non hate is even across the different datasets (train, test, validation). Since it is a binary classification, the distribution is important to adress when it comes to our accuracy scores. With 42% hate distribution, a model that purely guesses hate (or is overly tuned to guess hate) will receive an accuracy score of 42%. Conversely, it would get a 58% score if it guessed only non hate.\n",
    "\n",
    "In the sentiment dataset the distribution between the test, validation and train seem to differ. The train and validation sets have a similar distribtuion, while the test has a large shift in the negative and positive counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_hate_tokens = train_tokens_hate.to_numpy().tolist()\n",
    "list_sentiment_tokens = train_tokens_sentiment.to_numpy().tolist()\n",
    "\n",
    "train_list_hate = [i for sublist in list_hate_tokens for item in sublist for i in item]\n",
    "train_list_sentiment = [i for sublist in list_sentiment_tokens for item in sublist for i in item]\n",
    "\n",
    "\n",
    "n = 100000\n",
    "counter_train = Counter(train_list_hate)\n",
    "counter_sentiment = Counter(train_list_sentiment)\n",
    "counter_list = [counter_train, counter_sentiment]\n",
    "for i in range(len(counter_list)):\n",
    "    one_ = []\n",
    "    two_ = []\n",
    "    three_ = [] \n",
    "    for key, item in counter_list[i].items():\n",
    "        if item == 2:\n",
    "            two_.append((key, item))\n",
    "        if item == 1:\n",
    "            one_.append((key, item))\n",
    "        if item == 3:\n",
    "            three_.append((key, item))\n",
    "    print(texts[i], '\\n')\n",
    "    print(counter_list[i].most_common(10), '\\n')\n",
    "    print(one_[:50], '\\n')\n",
    "    print(two_[:50], '\\n')\n",
    "    print(three_[:50], '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The least frequent words are nouns and proper nouns, which makes sense - they are usually very specific as they describe specific things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Counter(train_list_hate).most_common()[::-1]\n",
    "print(f\"HATE\\n\\nvocabulary_size: {len(x)}\\ncorpus size: {len(train_list_hate)}\\ntypes/tokens ratio: {len(x)/len(train_list_hate)}\")\n",
    "y = Counter(train_list_sentiment).most_common()[::-1]\n",
    "print(f\"\\nSENTIMENT\\n\\nvocabulary_size: {len(y)}\\ncorpus size: {len(train_list_sentiment)}\\ntypes/tokens ratio: {len(y)/len(train_list_sentiment)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences: The sentiments corpus size is significantly bigger, which produces a better token/type ratio (more tokens are repeated words). However, we didn't notice any major differences with respect to the least frequent tokens. They were still mostly nouns. However, there was a big difference in the numbers of the most frequent nouns due to the corpus size - the Zipf's Law can be seen here in action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipfs law\n",
    "\n",
    "Zipf's law is a phenonoma found in statistics, originating from the relationship of word rank and occurences. In essence, as rank increases, word occurence decreases in a log-like relationship. The following piece of code seeks to visualize this phenomena. It plots the log log plot of the words' ranks at the x axis, and amount of occurences on the y axis to show the inverse relationship between the two, along with a cumulative frequency plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_tokeized_dataset(dataset):\n",
    "    tok = TreebankWordTokenizer()\n",
    "\n",
    "    corpus = []\n",
    "    with open(f'data/{dataset}/train_text.txt', \"r\") as f:\n",
    "        corpus.extend(t for line in f for t in tok.tokenize(line))\n",
    "\n",
    "    with open(f\"data/{dataset}-list.pickle\", \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "\n",
    "def load_pickled_dataset(dataset):\n",
    "    if not exists(f\"data/{dataset}-list.pickle\"):\n",
    "        pickle_tokeized_dataset(dataset)\n",
    "    with open(f\"data/{dataset}-list.pickle\", \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def plot_zipf_distribution(dataset):\n",
    "    corpus = load_pickled_dataset(dataset)\n",
    "\n",
    "    voc = collections.Counter(corpus)\n",
    "    frq = pd.DataFrame(voc.most_common(), columns=[\"token\", \"frequency\"])\n",
    "\n",
    "    # Index in the sorted list\n",
    "    frq[\"idx\"] = frq.index + 1\n",
    "\n",
    "    # Frequency normalised by corpus size\n",
    "    frq[\"norm_freq\"] = frq.frequency / len(corpus)\n",
    "\n",
    "    # Cumulative normalised frequency\n",
    "    frq[\"cumul_frq\"] = frq.norm_freq.cumsum()\n",
    "\n",
    "    seaborn.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    fig.suptitle(f\"Zipf's Law for tokens in {dataset} dataset\", fontsize=16)\n",
    "\n",
    "    idx = np.array(frq.index.tolist(), dtype='float')\n",
    "\n",
    "    # Plot: Cumulative frequency by index\n",
    "    seaborn.scatterplot(ax = axes[0], x=idx, y=\"cumul_frq\", data=frq, edgecolor=\"none\")\n",
    "    axes[0].set_xlabel(\"Token Index\")\n",
    "    axes[0].set_ylabel(\"Cumulative Frequency\")\n",
    "    # Plot: Cumulative frequency by index, top 10000 tokens\n",
    "    #seaborn.relplot(x=\"idx\", y=\"cumul_frq\", data=frq[:10000], edgecolor=\"none\")\n",
    "    \n",
    "    #plt.show()\n",
    "\n",
    "    # Plot: Log-log plot for Zipf's law\n",
    "    frq[\"log(frequency)\"] = np.log(frq.frequency)\n",
    "    frq[\"log(rank)\"] = np.log(frq.frequency.rank(ascending=False))\n",
    "    seaborn.scatterplot(ax = axes[1], x=\"log(rank)\", y=\"log(frequency)\", data=frq, edgecolor=\"none\", palette= \"flare\")\n",
    "    axes[1].set_xlabel(\"Log Rank\")\n",
    "    axes[1].set_ylabel(\"Log Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['hate', 'sentiment']\n",
    "\n",
    "for i in dataset:\n",
    "    print(f'\\n {i} distributions \\n' )\n",
    "    plot_zipf_distribution(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the plotted graphs, for both corpora follow zipf's law. As rank increases, frequency decreases in a log like relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Inter-Annotator Agreement (IAA) based on annotated data in data/hate/samples/ folder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the annotated data for each coder\n",
    "alex = pd.read_csv('data/hate/samples/alex_annotations.csv', sep='\\t')\n",
    "ea = pd.read_csv('data/hate/samples/EA_eval.csv')\n",
    "malthe = pd.read_csv('data/hate/samples/malthe_annotations.csv', sep='\\t')\n",
    "christian = pd.read_csv('data/hate/samples/christian_annotations.csv', sep='\\t')\n",
    "krz = pd.read_csv('data/hate/samples/krzysztof_parocki_annotations.csv', sep='\\t')\n",
    "true = pd.read_csv('data/hate/samples/train_text_sample_with_labels.csv', sep='\\t')\n",
    "\n",
    "# Create a new dataframe with only the text column\n",
    "df = pd.DataFrame(columns=['text'])\n",
    "df['text'] = alex['text']\n",
    "\n",
    "# Then add the separate labels from each coder\n",
    "df['alex'] = alex['label']\n",
    "df['christian'] = christian['label']\n",
    "df['krz'] = krz['label']\n",
    "df['ea'] = ea['label']\n",
    "df['malthe'] = malthe['label']\n",
    "df['true'] = true['label']\n",
    "\n",
    "# Create a new df with the columns [coder, item, label]\n",
    "new_df = pd.DataFrame(columns=['coder', 'item', 'label'])\n",
    "\n",
    "# For each unique text in df create a new row in new_df\n",
    "for index, row in df.iterrows():\n",
    "    new_df = new_df.append({'coder': 'alex', 'item': row['text'], 'label': row['alex']}, ignore_index=True)\n",
    "    new_df = new_df.append({'coder': 'christian', 'item': row['text'], 'label': row['christian']}, ignore_index=True)\n",
    "    new_df = new_df.append({'coder': 'krz', 'item': row['text'], 'label': row['krz']}, ignore_index=True)\n",
    "    new_df = new_df.append({'coder': 'ea', 'item': row['text'], 'label': row['ea']}, ignore_index=True)\n",
    "    new_df = new_df.append({'coder': 'malthe', 'item': row['text'], 'label': row['malthe']}, ignore_index=True)\n",
    "\n",
    "# The format of the data should be: (coder, item, label)\n",
    "# Turn the new_df into a list of tuples\n",
    "new_df_list = new_df.values.tolist()\n",
    "# Turn list of lists into list of tuples\n",
    "new_df_list = [tuple(x) for x in new_df_list]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    new_df = new_df.append({'coder': 'true', 'item': row['text'], 'label': row['true']}, ignore_index=True)\n",
    "\n",
    "all_df_list = new_df.values.tolist()\n",
    "# Turn list of lists into list of tuples\n",
    "all_df_list = [tuple(x) for x in all_df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAA = AnnotationTask(data=new_df_list)\n",
    "IAA2 = AnnotationTask(data=all_df_list)\n",
    "\n",
    "print(f\"Inter Annotator Agreement (without true labels): {IAA.pi()}\")\n",
    "print(f\"Inter Annotator Agreement (with true labels): {IAA2.pi()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inter Annotator Agreement was quite bad, especially after taking into consideration the actual markings (0.21). Without them, the value is noticeably (0.29) higher. That means we encountered difficulties with understanding guidelines during the annotation task, and we probably should do it again if we want to obtain usable data. However, we decided to use the official labels for the classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Prediction\n",
    "\n",
    "We will use Scikit-learn to create a pipeline to preprocess and classify text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data:\n",
    "dataset = 'hate'\n",
    "df_train = load_to_pd('train', dataset)\n",
    "df_test = load_to_pd('test', dataset)\n",
    "df_val = load_to_pd('val', dataset)\n",
    "\n",
    "# Combine train and test\n",
    "df_all = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "porter = PorterStemmer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def preprocess(tweet_text):\n",
    "    # Lowercase\n",
    "    tweet_text = tweet_text.lower()\n",
    "\n",
    "    # Remove punctuation using regex\n",
    "    tweet_text = re.sub(r'[^\\w\\s]', '', tweet_text)\n",
    "\n",
    "    # Remove stopwords from string\n",
    "    tweet_text = ' '.join([word for word in tweet_text.split() if word not in english_stopwords])\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmed = [porter.stem(word) for word in tweet_text.split(' ')]\n",
    "\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def tokenize(tweet_text):\n",
    "    return tk.tokenize(tweet_text)\n",
    "\n",
    "# Test the above functions\n",
    "tokenize(preprocess(\"Hello @user. This is a very LONG test sentence with. and ! ? #maga\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(stop_words=None, preprocessor=preprocess, tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SGDClassifier(random_state = 1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using grid search to find the optimal parameters for the pipeline components\n",
    "parameters = {\n",
    "    'vect__max_df': (0.7, 1.0),\n",
    "    'vect__min_df': (0, 0.05),\n",
    "    'vect__max_features': (None, 5000, 10000),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    # 'clf__loss': ('hinge', 'log'), # loss function of sgd classifier\n",
    "    # 'clf__alpha': (0.00001, 0.0001, 0.001),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=3)\n",
    "print(\"Performing grid search...\")\n",
    "grid_search.fit(df_train['text'], df_train['label'])\n",
    "print(\"Grid search complete!\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(f\"{param_name}: {best_parameters[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Below are the best parameters found for the pipeline using the hate dataset:\n",
    "    Best score: 0.7584444444444445\n",
    "    Best parameters set:\n",
    "    clf__alpha: 1e-04\n",
    "    clf__loss: log\n",
    "    tfidf__use_idf: False\n",
    "    vect__max_df: 1.0\n",
    "    vect__max_features: 10000\n",
    "    vect__min_df: 0\n",
    "\n",
    "For the sentiment is is: \n",
    "    Best score: 0.6477912967225693\n",
    "    Best parameters set:\n",
    "    tfidf__use_idf: True\n",
    "    vect__max_df: 1.0\n",
    "    vect__max_features: 10000\n",
    "    vect__min_df: 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes in a model and outputs the performance\n",
    "def performance(model, X, y, _dataset='hate'):\n",
    "    y_pred = model.predict(X)\n",
    "    if _dataset == 'hate':\n",
    "        target_names = ['not hate', 'hate']\n",
    "    elif _dataset == 'sentiment':\n",
    "        target_names = ['negative', 'neutral', 'positive']\n",
    "    print(classification_report(y, y_pred, target_names=target_names))\n",
    "    return model.score(X, y)\n",
    "    \n",
    "performance(grid_search, df_val['text'], df_val['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting hate speech\n",
    "\n",
    "#not hate example\n",
    "not_hate_string = \"This is a test sentence that is not hate.\"\n",
    "print(grid_search.predict([not_hate_string]))\n",
    "\n",
    "#hate example\n",
    "hate_string = \"all immigrants should rot in jail #kill #MAGA\"\n",
    "print(grid_search.predict([hate_string]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for sentiment classification\n",
    "# Create pipeline\n",
    "sentiment_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(stop_words=None, preprocessor=preprocess, tokenizer=tokenize, max_features=10000, min_df=0)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SGDClassifier(random_state = 1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on sentiment dataset\n",
    "sentiment_pipeline.fit(train_df_sentiment['text'], train_df_sentiment['label'])\n",
    "\n",
    "# Report performance\n",
    "performance(sentiment_pipeline, test_df_sentiment['text'], test_df_sentiment['label'], _dataset='sentiment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "We chose to augment our data by additional tweets from the offensive dataset, which seemed to contain some good sentences also for task due to similarities to the hate dataset. We tested data augmentation only on the hate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for \n",
    "FILE_PATH_HATE = 'data/hate/train_text.txt'\n",
    "FILE_PATH_OFFENSIVE = 'data/offensive/train_text.txt'\n",
    "\n",
    "def tokenize_lines_list(file_path):\n",
    "    # Helper function to tokenize lines for n-grams\n",
    "    line_list = []\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line)\n",
    "        line_list.append(tokens)\n",
    "    \n",
    "    return line_list\n",
    "\n",
    "tokenized_lines = tokenize_lines_list(FILE_PATH_HATE)\n",
    "tokenized_lines_offensive = tokenize_lines_list(FILE_PATH_OFFENSIVE)\n",
    "training_data = tokenized_lines\n",
    "validation_data = tokenized_lines_offensive\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "# for cutoff\n",
    "one_occurance_tokens = set()\n",
    "\n",
    "for line in training_data:\n",
    "    all_tokens.extend(line)\n",
    "\n",
    "freq_dict = Counter(all_tokens)\n",
    "\n",
    "for token in freq_dict.keys():\n",
    "    if freq_dict[token] <= 2:\n",
    "        one_occurance_tokens.add(token)\n",
    "\n",
    "\n",
    "print(training_data[0], '\\n')\n",
    "\n",
    "# cutoff = 2 has an effect on perplexity by reducing the number of considered options.\n",
    "# The removed words get the same prob as new words.\n",
    "\n",
    "for i, line in enumerate(training_data):\n",
    "    for j, token in enumerate(line):\n",
    "        if token in one_occurance_tokens:\n",
    "            training_data[i][j] = '<UNK>'\n",
    "\n",
    "print(training_data[0])\n",
    "\n",
    "#training the model on the distribution after cutoff\n",
    "train, vocab = padded_everygram_pipeline(3, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the language model using WittenBell with backoff\n",
    "# We weren't able to implement KneserNey due to bugs with infinite perplexities\n",
    "lm = WittenBellInterpolated(3)\n",
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.score(\"a\"))\n",
    "print(lm.vocab)\n",
    "print(lm.counts)\n",
    "print(lm.generate(40, random_seed=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_bigrams = []\n",
    "validation_data_trigrams = []\n",
    "\n",
    "for i in validation_data:\n",
    "    validation_data_bigrams.append(list(bigrams(i)))\n",
    "    validation_data_trigrams.append(list(trigrams(i)))\n",
    "\n",
    "\n",
    "print(validation_data_bigrams[0])\n",
    "print(validation_data_trigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report bigram perplexity\n",
    "for line in validation_data_bigrams: \n",
    "    if len(line) == 0:\n",
    "        validation_data_bigrams.remove(line)\n",
    "\n",
    "print(lm.perplexity(validation_data_bigrams[0]))\n",
    "\n",
    "# Report the trigram perplexity\n",
    "for line in validation_data_trigrams: \n",
    "    if len(line) == 0:\n",
    "        validation_data_trigrams.remove(line)\n",
    "\n",
    "print(lm.perplexity(validation_data_trigrams[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_perplexity = []\n",
    "\n",
    "# We will be using trigrams from now on as they provide more context\n",
    "for line in validation_data_trigrams: \n",
    "    list_of_perplexity.append(lm.perplexity(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report some key statistics\n",
    "mean_perplexity = np.mean(list_of_perplexity)\n",
    "median_perplexity = sorted(list_of_perplexity)[len(list_of_perplexity)//2]\n",
    "mean_perplexity, median_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the mean is considerably higher than the median, the distribution has to be strongly right-skewed. This is good for us, as we will extract only the left-most sentences with low perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram over perplexity\n",
    "plt.hist(list_of_perplexity, bins = 50, range =(0,2000))\n",
    "plt.xlabel('Perplexity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Perplexity Histogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the first quantile as the threshold for self-training sentences. This will add around 4000 sentences to our training set, which is more than 1/3 of the original training set. After looking at the histogram and the mean/median difference, we suspect it will have a positive impact on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first quantile\n",
    "quantile = np.percentile(list_of_perplexity, 25)\n",
    "\n",
    "\n",
    "# Get the trigram sentences index that have a perplexity below the quantile\n",
    "quantile_sentences = [index for index, sent in enumerate(validation_data_bigrams) if lm.perplexity(sent) < quantile]\n",
    "quantile, len(quantile_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences that have a perplexity below the quantile to a file\n",
    "with open(FILE_PATH_OFFENSIVE, 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Save lines to a new file if they have a perplexity below the quantile (trigrams)\n",
    "with open('data/hate/offensive_sentences_trigram_text.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for index in quantile_sentences:\n",
    "        f.write(lines[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load offensive dataset\n",
    "df_offensive = load_augmented_df('offensive_sentences_trigram', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels for offensive sentences\n",
    "df_offensive['label'] = grid_search.predict(df_offensive['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 random sentences from the offensive dataset - for insight\n",
    "df_offensive[df_offensive['label'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the offensive dataset\n",
    "\n",
    "# Concat the offensive dataset with the training dataset\n",
    "df_all_train = pd.concat([df_train, df_offensive])\n",
    "\n",
    "model = pipeline.fit(df_all_train['text'], df_all_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new performance\n",
    "performance(model, df_val['text'], df_val['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After adding trigram augmented data the accuracy increased along with the precision and recall metrics. In our case, recall is measured as properly labelled non-hate tweets, and precision as how many of non-hate labellings were correct. Therefore, it would make sense for recall to increase and precision to fall - by using the augmented dataset, we mostly provided our model with more examples of non-hate tweets. This was indeed the case for other random states. However, with this one, the accuracy increased so much that both precision and recall metrics went up. Therefore, we credit this double-increase mostly to hyperparameter (in this case, random state) tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69279e2190e015e8b10176866f54925f7e74c2766a0991ccefc0132916261cb7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fyp3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
