{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kparo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from collections import Counter\n",
    "from nltk.lm import Laplace, KneserNeyInterpolated, WittenBellInterpolated\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "from nltk import FreqDist, KneserNeyProbDist\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.api import LanguageModel, Smoothing\n",
    "from nltk.lm.smoothing import AbsoluteDiscounting, KneserNey, WittenBell\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/hate/train_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lines_list(file_path):\n",
    "    line_list = []\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line)\n",
    "        line_list.append(tokens)\n",
    "    \n",
    "    return line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "7200\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "tokenized_lines = tokenize_lines_list(FILE_PATH)\n",
    "training_size = (len(tokenized_lines) // 5) * 4 #80% of the original dataset\n",
    "\n",
    "training_data = tokenized_lines[:training_size] #slice to training data\n",
    "# for line in tokenized_lines[:training_size]:\n",
    "#     training_data.extend(line)\n",
    "\n",
    "c = Counter()\n",
    "for line in training_data:\n",
    "    c.update(line) #count words (tokens) freq count of tokens\n",
    "\n",
    "validation_data = tokenized_lines[- (len(tokenized_lines) - training_size):] #take 20% from the end\n",
    "print(len(tokenized_lines))\n",
    "print(len(training_data))\n",
    "print(len(validation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@', 'user', 'nice', 'new', 'signage', '.', 'Are', 'you', 'not', 'concerned', 'by', 'Beatlemania', '-style', 'hysterical', 'crowds', 'crongregating', 'on', 'you…']]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 5216), ('@', 5002), ('user', 4875), ('.', 4571), ('the', 3794)]\n"
     ]
    }
   ],
   "source": [
    "print(c.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(n, training_data)\n",
    "\n",
    "model = KneserNeyInterpolated(n) \n",
    "model.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1799"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = [[word for word in bigrams(line)] for line in validation_data] #for each sentece, create bigrams\n",
    "valid = [sent for sent in valid if sent] #return a sentence if it's not empty. If its empty if False, and sent gets discarded\n",
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lst_perplexity = []\n",
    "for sent in tqdm(valid[:10]):\n",
    "    lst_perplexity.append(model.perplexity(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trigram grouped sentences</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(#, WorldCup), (WorldCup, Blog), (Blog, for),...</td>\n",
       "      <td>1136.438943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(The, ðŸ‡ªðŸ‡ºEU), (ðŸ‡ªðŸ‡ºEU, discuss), (di...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(@, user), (user, Thank), (Thank, you), (you,...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(@, user), (user, @), (@, user), (user, Pay),...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Prom, is), (is, coming), (coming, up), (up, ...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(Sptingtime, isnt), (isnt, far), (far, .), (....</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(Read, World), (World, Relief), (Relief, 's),...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(ICE, protester), (protester, who), (who, sca...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(@, user), (user, ``), (``, 83), (83, %), (%,...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(``, Differences), (Differences, are), (are, ...</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Trigram grouped sentences   perplexity\n",
       "0  [(#, WorldCup), (WorldCup, Blog), (Blog, for),...  1136.438943\n",
       "1  [(The, ðŸ‡ªðŸ‡ºEU), (ðŸ‡ªðŸ‡ºEU, discuss), (di...          inf\n",
       "2  [(@, user), (user, Thank), (Thank, you), (you,...          inf\n",
       "3  [(@, user), (user, @), (@, user), (user, Pay),...          inf\n",
       "4  [(Prom, is), (is, coming), (coming, up), (up, ...          inf\n",
       "5  [(Sptingtime, isnt), (isnt, far), (far, .), (....          inf\n",
       "6  [(Read, World), (World, Relief), (Relief, 's),...          inf\n",
       "7  [(ICE, protester), (protester, who), (who, sca...          inf\n",
       "8  [(@, user), (user, ``), (``, 83), (83, %), (%,...          inf\n",
       "9  [(``, Differences), (Differences, are), (are, ...          inf"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(valid, lst_perplexity)),columns = ['Trigram grouped sentences','perplexity'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
