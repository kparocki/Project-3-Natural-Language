{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from collections import Counter\n",
    "from nltk.lm import Laplace, KneserNeyInterpolated, WittenBellInterpolated\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Malth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.lm import MLE, Vocabulary\n",
    "from nltk.lm.api import LanguageModel, Smoothing\n",
    "from nltk.lm.smoothing import AbsoluteDiscounting, KneserNey, WittenBell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/hate/train_text.txt'\n",
    "FILE_PATH_2 = 'data/offensive/train_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lines_list(file_path):\n",
    "    line_list = []\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line)\n",
    "        line_list.append(tokens)\n",
    "    \n",
    "    return line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(FILE_PATH, 'r', encoding = 'utf-8') as f:\n",
    "#     dataset = f.readlines()\n",
    "    \n",
    "# split = round(len(dataset)*0.8)\n",
    "# train_data = dataset[:split]\n",
    "# test_data = dataset[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "#                 for sent in train_data]\n",
    "\n",
    "# validation_data = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "#                 for sent in test_data]\n",
    "\n",
    "# validation_data = [sent for sent in validation_data if sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'user', 'nice', 'new', 'signage', '.', 'Are', 'you', 'not', 'concerned', 'by', 'Beatlemania', '-style', 'hysterical', 'crowds', 'crongregating', 'on', 'you…'] \n",
      "\n",
      "['@', 'user', 'nice', 'new', '<UNK>', '.', 'Are', 'you', 'not', 'concerned', 'by', '<UNK>', '<UNK>', 'hysterical', '<UNK>', '<UNK>', 'on', 'you…']\n"
     ]
    }
   ],
   "source": [
    "tokenized_lines = tokenize_lines_list(FILE_PATH)\n",
    "tokenized_lines_offensive = tokenize_lines_list(FILE_PATH_2)\n",
    "training_data = tokenized_lines\n",
    "validation_data = tokenized_lines_offensive\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "# for cutoff\n",
    "one_occurance_tokens = set()\n",
    "\n",
    "for line in training_data:\n",
    "    all_tokens.extend(line)\n",
    "\n",
    "freq_dict = Counter(all_tokens)\n",
    "\n",
    "for token in freq_dict.keys():\n",
    "    if freq_dict[token] <= 2:\n",
    "        one_occurance_tokens.add(token)\n",
    "\n",
    "\n",
    "print(training_data[0], '\\n')\n",
    "\n",
    "# cutoff = 1. Has an effect on perplexity by reducing the number of considered options.\n",
    "# The removed words get the same prob as new words (approximated by a constant)\n",
    "\n",
    "for i, line in enumerate(training_data):\n",
    "    for j, token in enumerate(line):\n",
    "        if token in one_occurance_tokens:\n",
    "            training_data[i][j] = '<UNK>'\n",
    "\n",
    "print(training_data[0])\n",
    "\n",
    "#let's actually train the model on the distribution after cutoff\n",
    "train, vocab = padded_everygram_pipeline(3, training_data)\n",
    "\n",
    "# print(training_data[1])\n",
    "\n",
    "# l = []\n",
    "# for i in range (1000):\n",
    "#     l.append(next(vocab))\n",
    "\n",
    "# len(l)\n",
    "# s = set()\n",
    "\n",
    "# for i in range(1):\n",
    "#     j = next(train)\n",
    "#     for _ in range(30):\n",
    "#         print(next(j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WittenBellInterpolated(3)\n",
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013959540991364013\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 6307 items>\n",
      "<NgramCounter with 3 ngram orders and 759129 ngrams>\n",
      "['<UNK>', 'all', 'kind', 'of', 'negativity', 'in', 'my', 'phone', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm.score(\"a\"))\n",
    "print(lm.vocab)\n",
    "print(lm.counts)\n",
    "print(lm.generate(40, random_seed=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@', 'user'), ('user', 'Bono'), ('Bono', '...'), ('...', 'who'), ('who', 'cares'), ('cares', '.'), ('.', 'Soon'), ('Soon', 'people'), ('people', 'will'), ('will', 'understand'), ('understand', 'that'), ('that', 'they'), ('they', 'gain'), ('gain', 'nothing'), ('nothing', 'from'), ('from', 'following'), ('following', 'a'), ('a', 'phony'), ('phony', 'celebrity'), ('celebrity', '.'), ('.', 'Become'), ('Become', 'a'), ('a', 'Leader'), ('Leader', 'of'), ('of', 'your'), ('your', 'people'), ('people', 'instead'), ('instead', 'or'), ('or', 'help'), ('help', 'and'), ('and', 'support'), ('support', 'your'), ('your', 'fellow'), ('fellow', 'countrymen'), ('countrymen', '.')]\n",
      "[('@', 'user', 'Bono'), ('user', 'Bono', '...'), ('Bono', '...', 'who'), ('...', 'who', 'cares'), ('who', 'cares', '.'), ('cares', '.', 'Soon'), ('.', 'Soon', 'people'), ('Soon', 'people', 'will'), ('people', 'will', 'understand'), ('will', 'understand', 'that'), ('understand', 'that', 'they'), ('that', 'they', 'gain'), ('they', 'gain', 'nothing'), ('gain', 'nothing', 'from'), ('nothing', 'from', 'following'), ('from', 'following', 'a'), ('following', 'a', 'phony'), ('a', 'phony', 'celebrity'), ('phony', 'celebrity', '.'), ('celebrity', '.', 'Become'), ('.', 'Become', 'a'), ('Become', 'a', 'Leader'), ('a', 'Leader', 'of'), ('Leader', 'of', 'your'), ('of', 'your', 'people'), ('your', 'people', 'instead'), ('people', 'instead', 'or'), ('instead', 'or', 'help'), ('or', 'help', 'and'), ('help', 'and', 'support'), ('and', 'support', 'your'), ('support', 'your', 'fellow'), ('your', 'fellow', 'countrymen'), ('fellow', 'countrymen', '.')]\n"
     ]
    }
   ],
   "source": [
    "validation_data_bigrams = []\n",
    "validation_data_trigrams = []\n",
    "\n",
    "for i in validation_data:\n",
    "    validation_data_bigrams.append(list(bigrams(i)))\n",
    "    validation_data_trigrams.append(list(trigrams(i)))\n",
    "\n",
    "\n",
    "print(validation_data_bigrams[0])\n",
    "print(validation_data_trigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.6872211081506"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigram perplexity\n",
    "for line in validation_data_bigrams: \n",
    "    if len(line) == 0:\n",
    "        validation_data_bigrams.remove(line)\n",
    "\n",
    "lm.perplexity(validation_data_bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444.6542185531703"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trigram perplexity\n",
    "for line in validation_data_trigrams: \n",
    "    if len(line) == 0:\n",
    "        validation_data_trigrams.remove(line)\n",
    "\n",
    "lm.perplexity(validation_data_trigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_perplexity = []\n",
    "\n",
    "for line in validation_data_trigrams: \n",
    "    list_of_perplexity.append(lm.perplexity(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2781.9411406659824, 208.1023568762775)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_perplexity = sum(list_of_perplexity) / len(list_of_perplexity)\n",
    "median_perplexity = sorted(list_of_perplexity)[len(list_of_perplexity)//2]\n",
    "mean_perplexity, median_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram grouped sentences</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(@, user), (user, Bono), (Bono, ...), (..., w...</td>\n",
       "      <td>444.654219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(@, user), (user, Eight), (Eight, years), (ye...</td>\n",
       "      <td>441.577809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(@, user), (user, Get), (Get, him), (him, som...</td>\n",
       "      <td>536.712172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(@, user), (user, @), (@, user), (user, She),...</td>\n",
       "      <td>45.551715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(@, user), (user, She), (She, has), (has, bec...</td>\n",
       "      <td>389.909470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11911</th>\n",
       "      <td>[(@, user), (user, I), (I, wonder), (wonder, i...</td>\n",
       "      <td>233.717542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11912</th>\n",
       "      <td>[(@, user), (user, Do), (Do, we), (we, dare), ...</td>\n",
       "      <td>537.141481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>[(@, user), (user, No), (No, idea), (idea, who...</td>\n",
       "      <td>721.260618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11914</th>\n",
       "      <td>[(#, Professor), (Professor, Who), (Who, Shot)...</td>\n",
       "      <td>622.734027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>[(@, user), (user, @), (@, user), (user, @), (...</td>\n",
       "      <td>422.951664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11916 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                bigram grouped sentences  perplexity\n",
       "0      [(@, user), (user, Bono), (Bono, ...), (..., w...  444.654219\n",
       "1      [(@, user), (user, Eight), (Eight, years), (ye...  441.577809\n",
       "2      [(@, user), (user, Get), (Get, him), (him, som...  536.712172\n",
       "3      [(@, user), (user, @), (@, user), (user, She),...   45.551715\n",
       "4      [(@, user), (user, She), (She, has), (has, bec...  389.909470\n",
       "...                                                  ...         ...\n",
       "11911  [(@, user), (user, I), (I, wonder), (wonder, i...  233.717542\n",
       "11912  [(@, user), (user, Do), (Do, we), (we, dare), ...  537.141481\n",
       "11913  [(@, user), (user, No), (No, idea), (idea, who...  721.260618\n",
       "11914  [(#, Professor), (Professor, Who), (Who, Shot)...  622.734027\n",
       "11915  [(@, user), (user, @), (@, user), (user, @), (...  422.951664\n",
       "\n",
       "[11916 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(validation_data_bigrams, list_of_perplexity)),columns = ['bigram grouped sentences','perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create histogram over perplexity\n",
    "\n",
    "plt.hist(list_of_perplexity, bins = 50, range =(0,2000))\n",
    "plt.xlabel('Perplexity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Perplexity Histogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97.0490484022697, 4377)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will take the first quantile as the threshold for self-training\tsentences, this will add around 3000 sentences to \n",
    "#our training set, which is 1/3 of the original training set. Also, looking at the hist, it seems like \n",
    "#a good idea\n",
    "\n",
    "# Calculate the first quantile\n",
    "quantile = np.percentile(list_of_perplexity, 25)\n",
    "\n",
    "\n",
    "# Get the sentences index that have a perplexity below the quantile\n",
    "quantile_sentences = [index for index, sent in enumerate(validation_data_bigrams) if lm.perplexity(sent) < quantile]\n",
    "quantile, len(quantile_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the sentences that have a perplexity below the quantile to a file\n",
    "# with open(FILE_PATH_2, 'r', encoding = 'utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# # Save lines to a new file if they have a perplexity below the quantile\n",
    "# with open('data/hate/offensive_sentences_text.txt', 'w') as f:\n",
    "#     for index in quantile_sentences:\n",
    "#         f.write(lines[index])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9afbf2a9e3cca782218678ea53de780cdab6e415deb2cfebfa74b36b58b3a78"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
