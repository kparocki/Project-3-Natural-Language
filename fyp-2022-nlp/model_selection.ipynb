{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "from utils import load_to_pd, load_augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Malth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data:\n",
    "dataset = 'hate'\n",
    "df_train = load_to_pd('train', dataset)\n",
    "df_test = load_to_pd('test', dataset)\n",
    "df_val = load_to_pd('val', dataset)\n",
    "# Load offensive dataset\n",
    "df_offensive = load_augmented_df('offensive_sentences_bigram', dataset)\n",
    "\n",
    "# Combine train and test\n",
    "df_all = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "porter = PorterStemmer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def preprocess(tweet_text):\n",
    "    # Lowercase\n",
    "    tweet_text = tweet_text.lower()\n",
    "\n",
    "    # Remove punctuation using regex\n",
    "    tweet_text = re.sub(r'[^\\w\\s]', '', tweet_text)\n",
    "\n",
    "    # Remove stopwords from string\n",
    "    tweet_text = ' '.join([word for word in tweet_text.split() if word not in english_stopwords])\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmed = [porter.stem(word) for word in tweet_text.split(' ')]\n",
    "\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def tokenize(tweet_text):\n",
    "    return tk.tokenize(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the above functions\n",
    "tokenize(preprocess(\"Hello @user. This is a very LONG test sentence with. and ! ? #maga\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(stop_words=None, preprocessor=preprocess, tokenizer=tokenize,)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SGDClassifier()),\n",
    "        # (\"clf\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the pipeline for testing purposes\n",
    "# pipeline.fit(df_train['text'], df_train['label'])\n",
    "\n",
    "# print(\"Pipeline with default arguments:\")\n",
    "# print(\"Pipeline:\", [name for name, _ in pipeline.steps])\n",
    "# print(\"Fitting model to training data...\")\n",
    "# pipeline.fit(df_train['text'], df_train['label'])\n",
    "# print(\"Model fitted!\")\n",
    "\n",
    "# print(\"Evaluating model...\")\n",
    "# print(f\"Model score: {pipeline.score(df_test['text'], df_test['label'])}\")\n",
    "# print(\"Model evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using grid search to find the optimal parameters for the pipeline components\n",
    "parameters = {\n",
    "    'vect__max_df': (0.7, 1.0),\n",
    "    'vect__min_df': (0, 0.05),\n",
    "    'vect__max_features': (None, 5000, 10000),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    # 'clf__loss': ('hinge', 'log'), # loss function of sgd classifier\n",
    "    # 'clf__alpha': (0.00001, 0.001),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=3)\n",
    "print(\"Performing grid search...\")\n",
    "grid_search.fit(df_train['text'], df_train['label'])\n",
    "print(\"Grid search complete!\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(f\"{param_name}: {best_parameters[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Below are the best parameters found for the pipeline using the hate dataset:\n",
    "    Best score: 0.7584444444444445\n",
    "    Best parameters set:\n",
    "    clf__alpha: 1e-05\n",
    "    clf__loss: log\n",
    "    tfidf__use_idf: False\n",
    "    vect__max_df: 1.0\n",
    "    vect__max_features: 10000\n",
    "    vect__min_df: 0\n",
    "\n",
    "For the sentiment is is: \n",
    "    Best score: 0.6477912967225693\n",
    "    Best parameters set:\n",
    "    tfidf__use_idf: True\n",
    "    vect__max_df: 1.0\n",
    "    vect__max_features: 10000\n",
    "    vect__min_df: 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_95164/3709646288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mperformance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "# A function that takes in a model and outputs the performance\n",
    "def performance(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    if dataset == 'hate':\n",
    "        target_names = ['not hate', 'hate']\n",
    "    elif dataset == 'sentiment':\n",
    "        target_names = ['negative', 'neutral', 'positive']\n",
    "    print(classification_report(y, y_pred, target_names=target_names))\n",
    "    return model.score(X, y)\n",
    "    \n",
    "performance(grid_search, df_val['text'], df_val['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting hate speech\n",
    "\n",
    "#not hate example\n",
    "not_hate_string = \"This is a test sentence that is not hate.\"\n",
    "grid_search.predict([not_hate_string])\n",
    "\n",
    "#hate example\n",
    "hate_string = \"all immigrants should rot in jail #kill\"\n",
    "grid_search.predict([hate_string])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with new offensive sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline model with the best found parameters\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(stop_words=None, preprocessor=preprocess, tokenizer=tokenize, max_df=1.0, min_df=0, max_features=10000)),\n",
    "        (\"tfidf\", TfidfTransformer(use_idf=False)),\n",
    "        (\"clf\", SGDClassifier(alpha=1e-05, loss='log')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = pipeline.fit(df_train['text'], df_train['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not hate       0.75      0.71      0.73       573\n",
      "        hate       0.64      0.68      0.66       427\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.69      0.70      0.69      1000\n",
      "weighted avg       0.70      0.70      0.70      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.698"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance(model, df_val['text'], df_val['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels for offensive sentences\n",
    "df_offensive['label'] = model.predict(df_offensive['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 random sentences from the offensive dataset\n",
    "df_offensive[df_offensive['label'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the offensive dataset\n",
    "\n",
    "# Concat the offensive dataset with the training dataset\n",
    "df_all_train = pd.concat([df_train, df_offensive])\n",
    "\n",
    "model = pipeline.fit(df_all_train['text'], df_all_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not hate       0.76      0.71      0.73       573\n",
      "        hate       0.64      0.70      0.67       427\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.70      0.70      0.70      1000\n",
      "weighted avg       0.71      0.70      0.70      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.703"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check new performance\n",
    "performance(model, df_val['text'], df_val['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of adding augmented data\n",
    "After adding the new augmented bigram data and training a new model with this data, we saw a tiny increase in accuracy (0.698 before and 0.703 with). Also the recall for not-hate stayed the same and precision increased by 0.01.\n",
    "\n",
    "After adding trigram augmented data accuracy score increased from 0.698 to 0.701. For none-hate recall is improved by 0.07, and precision decreased by 0.03. This could be because of the now broader non hateful training labels."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9afbf2a9e3cca782218678ea53de780cdab6e415deb2cfebfa74b36b58b3a78"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
