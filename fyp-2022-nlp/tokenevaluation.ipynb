{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Evaluation\n",
    "Evaluate different tokenization aproaches by compairing results of simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'irony'\n",
    "\n",
    "## Loading the datasets and corresponding labels into panda dataframes\n",
    "\n",
    "# Experienced some issues with loading directly into a pd datafra, so made function that went line by line\n",
    "def load_to_pd(_type):\n",
    "    x = list()\n",
    "    with open(f'data/{dataset}/{_type}_text.txt', 'r') as text:\n",
    "        for line in text:\n",
    "            x.append(line.strip('\\n'))\n",
    "    return pd.DataFrame(x), x\n",
    "\n",
    "# train_text_df = pd.read_csv(f'data/{dataset}/train_text.txt',sep='\\n', header=None)\n",
    "train_text_df, train_list = load_to_pd('train')\n",
    "train_labels_df = pd.read_csv(f'data/{dataset}/train_labels.txt',sep='\\n', header=None)\n",
    "train_df = pd.concat([train_text_df, train_labels_df], axis=1)\n",
    "train_df.columns = ['text', 'label']\n",
    "\n",
    "test_text_df, test_list = load_to_pd('test')\n",
    "test_labels_df = pd.read_csv(f'data/{dataset}/test_labels.txt',sep='\\n', header=None)\n",
    "test_df = pd.concat([test_text_df, test_labels_df], axis=1)\n",
    "test_df.columns = ['text', 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (Temp/ipykernel_3116/636113979.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\CHRIST~1\\AppData\\Local\\Temp/ipykernel_3116/636113979.py\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    return list(map(lemma, ())))\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "def lemma(tokenlst):\n",
    "    return [nltk.stem.WordNetLemmatizer().lemmatize(x, pos='v') for x in tokenlst]\n",
    "\n",
    "\n",
    "def preprocess(lst):\n",
    "    \n",
    "    lst = map(nltk.tokenize.word_tokenize, lst)\n",
    "\n",
    "    return list(map(lemma, ())))\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "print(preprocess(train_df['text'][0:2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clf</th>\n",
       "      <td>0.673925</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>0.636468</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear SVM</th>\n",
       "      <td>0.682216</td>\n",
       "      <td>0.628827</td>\n",
       "      <td>0.629301</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beyes</th>\n",
       "      <td>0.676747</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.634634</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            precision    recall  f1-score  support\n",
       "name                                              \n",
       "clf          0.673925  0.633929  0.636468      784\n",
       "linear SVM   0.682216  0.628827  0.629301      784\n",
       "beyes        0.676747  0.632653  0.634634      784"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lst = []\n",
    "model_lst = [('clf', SGDClassifier(loss='log', penalty='l2',\n",
    "                        alpha=1.9e-3, random_state=42,\n",
    "                        max_iter=600, tol=8)),\n",
    "             ('linear SVM', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=1.9e-3, random_state=42,\n",
    "                        max_iter=600, tol=8)),\n",
    "            ('beyes', MultinomialNB())\n",
    "            ]\n",
    "\n",
    "\n",
    "for model in model_lst:\n",
    "    text_clf = Pipeline([\n",
    "        #('prepocess', preprocess()),\n",
    "        ('vect', CountVectorizer(stop_words='english')),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        model\n",
    "    ])\n",
    "    pipeline_lst.append(text_clf)\n",
    "\n",
    "res_lst = []\n",
    "for pipeline in pipeline_lst:\n",
    "    pipeline.fit(train_df['text'], train_df['label'])   \n",
    "    predicted = pipeline.predict(test_df['text'])\n",
    "    name = list(pipeline.named_steps.keys())[2]\n",
    "    metric_dic = metrics.classification_report(test_df['label'], predicted, output_dict=True)['weighted avg']\n",
    "    metric_dic['name'] = name\n",
    "    res_lst.append(metric_dic)\n",
    "\n",
    "df = pd.DataFrame(res_lst)\n",
    "df.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lst[2].predict(['i dont fucking love you'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
